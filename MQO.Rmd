---
title: "MQO"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, cache = TRUE, fig.align = "center")

options(scipen = 999)
```

```{r Preambulo, echo = TRUE}

library(ggplot2)
library(gganimate)
library(magrittr)
library(dplyr)
library(kableExtra)

```


Utilizaremos o banco de dados **trees**. Esse banco de dados contém informações sobre o diâmetro em polegadas, altura em pés e volume em pés³ de 31 árvores *black cherry*.  

Além disso, os gráficos serão gerados utilizando o pacote `ggplot2`, com comentários pertinentes sobre as funções utilizadas. Os comandos do pacote `ggplot2` são inseridos em camadas, normalmente iniciando por uma função que indica o banco de dados utilizado, e posteriormente as demais funções. As variáveis que serão utilizadas para criar os gráficos devem sempre estar "mapeadas" para serem encontradas, e a maneira de fazer isso é inserindo-as dentro do argumento `aes()`. No exemplo abaixo, o primeiro nível de comando especifica o banco de dados utilizado. O segundo indica o tipo de gráfico a ser criado e mapeia as variáveis de interesse. O terceiro comando introduz os títulos dos eixos, do gráfico, etc. E o quarto comando aplica um tema pronto ao gráfico.



```{r Banco de dados, echo = TRUE}

data("trees")

# Transformar as variáveis para unidades comumente utilizadas
d <- trees$Girth/2.54     # polegadas para centímetros
ht <- trees$Height/3.281  # pés para metros
v <- trees$Volume/34.286  # pés cúbicos para metros cúbicos
d2h <- d^2*ht             

# 'd' é um vetor de diâmetros, e possui 31 elementos. Ou length(d)
d
length(d)

# O mesmo ocorre com ht, v e d2h.

# Agrupar tudo em um único data frame
dados <- data.frame(d,ht,v,d2h)

g.base <- ggplot(data = dados, 
                 aes(x = d2h, y = v)) +     #Introduz o dataframe utilizado
  
  geom_point() + #geom_point cria gráfico de pontos)
  
  labs(x = "d2h",          #Adiciona Títulos dos eixos e do gráfico
       y = "Volume (m³)",
       title = "Volume em função de d2h") + 
  
  theme_bw()                #Adiciona o tema Black and White


plot(g.base)

```



A figura acima demonstra a relação entre volume e $d^2h$. Essa relação é conhecida no meio florestal através do modelo de Spurr: $v = \beta_0 + \beta_1.d^2h$, e evidencia a relação linear entre as duas variáveis.

Para encontrar os parâmetros do modelo, basta utilizar a função `lm()`.

```{r linear simples, echo = TRUE}

spurr <- lm(v ~ d2h, data = dados)

summary(spurr)

coef(spurr)

g.base + labs(title = "Modelo de Spurr Ajustado") +
  geom_abline(intercept = coef(spurr)[1],  #Coeficiente linear  (b0)
              slope = coef(spurr)[2],      #Coeficiente angular (b1)
              color = "steelblue") 


```

Os valores de $\beta_0$ e $\beta_1$ encontrados foram de `r round(coef(spurr)[1],4)` e `r round(coef(spurr)[2],4)`, respectivamente.

Agora vamos tentar chegar nos mesmos parâmetros realizando o método dos mínimos quadrados ordinários passo a passo.

## Mínimos Quadrados Ordinários

O método dos mínimos quadrados ordinários é o principal método de estimativa dos parâmetros para regressões lineares, e tem como objetivo encontrar os parâmetros do modelo que resultem na menor soma de quadrados dos resíduos. Sua utilização depende de pressuposições que devem ser satisfeitas. São elas:

- **1.** Erro com média 0 e distribuição normal;
- **2.** Variância do erro constante;
- **3.** Independência dos resíduos.


A formulação tradicional do modelo linear simples é exemplificada abaixo,

$$
y = \beta_0 + \beta_1x + \epsilon
$$

Onde $\beta_0$ é o parâmetro linear, também chamado de intercepto ou constante, pois não depende de $x$; $\beta_1$ é o parâmetro angular ou coeficiente da variável $x$; $\epsilon$ é o erro, e representa a variação de $y$ que não é explicada pelo modelo. Outra formulação encontrada na literatura é $y = \alpha + \beta x + \epsilon$.



Os valores estimados ou preditos da variável $y$ são denotados por $\hat{y}$,

$$
\hat{y} = \hat{\beta_0} + \hat{\beta_1}x 
$$

Os parâmetros *verdadeiros* $\beta_i$ do modelo são desconhecidos, e em vez disso se utiliza a denotação $\hat{\beta_0}$ e $\hat{\beta_1}$ para definir os parâmetros *estimados* pela regressão. 


O erro $\epsilon$ nada mais é que a diferença entre o valor real observado $y$, e o valor estimado ou predito $\hat{y}$.

$$
\epsilon_i = y_i - \hat{y_i}
$$


Como citado anteriormente, o objetivo do método é encontrar os parâmetros que resultem na menor soma de quadrados de resíduos. Como os resíduos devem possuir média zero e distribuição normal, alguns resíduos são positivos e outros negativos, e seu somatório resultaria no valor zero. Daí a necessidade de elevar cada erro $\epsilon$ ao quadrado.

$$
S = \sum_{i=1}^{n} \epsilon_i^2 = min
$$

Com a substituição de alguns termos com as equações apresentadas acima, temos que

$$
S = \sum_{i=1}^{n} (y_i - \hat{y_i})^2
$$

$$
S = \sum_{i=1}^{n} (y_i - [\hat{\beta_0} + \hat{\beta_1}x_i])^2
$$

$$
S = \sum_{i=1}^{n} (y_i - \hat{\beta_0} - \hat{\beta_1}x_i)^2 = min
$$


Agora, se atribuíssemos algum valor para os parâmetros no modelo acima e calculássemos o somatório, isso resultaria em um valor S de Soma de Quadrados de Resíduos. Conforme os valores dos parâmetros mudam, então também a Soma de Quadrados de Resíduos (S) irá mudar.

```{r Variação de S, echo = TRUE}

#Criar uma função S que recebe os parâmetros x,y,a,b
S <- function(x,y,b0,b1) {sum((y-b0-b1*x)^2)}

#Essa função criada receberá 4 argumentos: um vetor x, um vetor y, um valor de b0 e um valor de b1. Como x e y são dois vetores e b0 e b1 são dois valores, o que a função fará é realizar a operação y - b0 - b1*x para os primeiros valores de x e y e elevar o resultado ao quadrado. A seguir, realiza a mesma operação para os segundos valores e eleva o resultado ao quadrado. Por fim, é requisitado por meio da função sum() que faça o somatório de todos esses resultados. Por fim, isso resultará em um único valor que é a soma do quadrado dos resíduos para aquele par de coeficientes b0 e b1 informados.

#Criar um vetor de possíveis valores de b0 e b1
b_0 <- seq(from = -0.5, to = 0.5, by = 0.01)  #De -0.5 até 0.5, ao passo de 0.01
b_1 <- seq(from = -0.5, to = 0.5, by = 0.01)

#Isso resulta em 101 possíveis valores de b0 e b1.
length(b_0) ; length(b_1)

#Criar vetor que receberá o resultado de S para cada possível valor de b0 e b1. Naturalmente, deve ter o mesmo comprimento do número de possíveis valores dos coeficientes.
S.est <- vector(length = length(b_0))  #Vetor de comprimento igual ao vetor b0

#Criar um loop para estimar cada valor de S baseado em cada valor de b0 e b1
for (i in 1:length(S.est)) {
  
  S.est[i] <- S(x = dados$d2h,  
                y = dados$v,
                b0 = b_0[i],
                b1 = b_1[i])
}

#O comando acima pega a função S() criada no início desse bloco de código, joga os primeiros valores de b0 e b1 dos vetores, calcula o valor de S e o guarda na primeira posição do vetor S.est. A seguir, pega todos valores das segundas posições dos vetores, joga na função S() criada e resgata o valor resultante, salvando na segunda posição do vetor S.est. Esse processo é repetido de 1:length(S.est), ou seja, do primeiro até o último valor dos vetores.


#Criando um data frame com as variáveis
df <- data.frame("b0" = b_0,
                 "b1" = b_1,
                 "S" = S.est)

g.S <- ggplot(data = df, aes(x = b0, y = S)) + 
  geom_line(aes(color = S.est), size = 1) +  # Cria uma reta conectando cada observação. Como são 101 pontos, o resultado terá uma aparência  de curva parabólica.
  labs(x = "Parâmetro b0", y = "S (m³)²", subtitle = "Efeito de b0 na Soma de Quadrado dos Resíduos") + 
  scale_color_gradient(low = "blue", high = "red", name = "S (m³)²") + 
  geom_segment(aes(x=-0.2,xend=0.2, y = min(S), yend = min(S)), color = "black") +
  theme_bw()



plot(g.S)

```

Na figura acima, percebe-se que quando $\hat{\beta_0}$ é próximo de -0.5, a soma de quadrado dos resíduos é alta, assim como perto de 0.5. Entretanto, quando $\hat{\beta_0}$ é próximo de 0, S atinge os menores valores. Se fizéssemos esse procedimento com $\hat{\beta_1}$, esse mesmo comportamento seria observado. Em outras palavras, existe um determinado valor de $\hat{\beta_0}$ e $\hat{\beta_1}$ que resultam no valor mínimo de S. Para encontrar os valores exatos, podemos fazer uso do cálculo diferencial.

Analisando a figura, percebemos que o ponto que resulta no menor valor de S é um ponto de mínima, e se trassarmos uma reta tangente à curva nesse ponto, essa reta será perfeitamente horizontal. 

Uma maneira adequada para encontrar pontos críticos em uma função é utilizando cálculo diferencial, sabendo que nos pontos onde a primeira derivada for igual a zero, então a tangente da função será perfeitamente horizontal, marcando assim um ponto de máxima ou mínima.

Para encontrarmos os valores de $\hat{\beta_0}$ e $\hat{\beta_1}$ que minimizam S, precisamos derivar a função S em relação a esses parâmetros. Esse procedimento é chamado de derivada parcial (denotado por $\partial$), e em linhas gerais tudo que não possuir relação com o parâmetro alvo é considerado como constante.

Primeiramente podemos abrir o polinômio da função $S$.

$$
S = \sum_{i=1}^{n} (y_i - \hat{\beta_0} - \hat{\beta_1}x_i)^2
$$

$$
S = \sum_{i=1}^{n} (y_i^2 - y_i\hat{\beta_0} - y_i\hat{\beta_i}x_i-y_i\hat{\beta_0} + \hat{\beta_0}^2 + \hat{\beta_0}\hat{\beta_1}x_i - y_i\hat{\beta_1}x_i + \hat{\beta_0}\hat{\beta_1}x_i + \hat{\beta_1}^2x_i^2)
$$


$$
S = \sum_{i=1}^{n} (y_i^2 + \hat{\beta_0}^2 + \hat{\beta_1}^2x_i^2 -2y_i\hat{\beta_0} - 2y_i\hat{\beta_1}x_i + 2\hat{\beta_0}\hat{\beta_1}x_i)
$$

----

----

### Derivada parcial em relação à $\hat{\beta_0}$

As regras de derivação são as mesmas de qualquer outra diferenciação. A única diferença é que, agora, a variável de interesse é $\hat{\beta_0}$, e tudo o mais é considerado constante. 


$$
\frac{\partial S}{\partial \hat{\beta_0}} = \sum_{i=1}^{n} (0 + \hat{\beta_0}^2 + 0 - 2y_i\hat{\beta_0} - 0 + 2\hat{\beta_0}\hat{\beta_1}x_i)
$$


$$
\frac{\partial S}{\partial \hat{\beta_0}} = \sum_{i=1}^{n} (0 + 2\hat{\beta_0} + 0 - 2y_i - 0 + 2\hat{\beta_1}x_i)
$$


$$
\frac{\partial S}{\partial \hat{\beta_0}} = \sum_{i=1}^{n} (2\hat{\beta_0} - 2y_i + 2\hat{\beta_1}x_i) 
$$

Igualando a 0

$$
\frac{\partial S}{\partial \hat{\beta_0}} = \sum_{i=1}^{n} (2\hat{\beta_0} - 2y_i + 2\hat{\beta_1}x_i) = 0
$$


$$
\frac{\partial S}{\partial \hat{\beta_0}} = 2\sum_{i=1}^{n} (\hat{\beta_0} - y_i + \hat{\beta_1}x_i) = 0
$$


Podemos ainda realizar algumas simplificações.

Dividindo por 2 e multiplicando por -1


$$
\frac{\displaystyle  2\sum_{i=1}^{n} (\hat{\beta_0} - y_i + \hat{\beta_1}x_i)}{2} (-1) = \frac{0}{2}(-1)
$$


$$
\sum_{i=1}^{n} (y_i - \hat{\beta_0} - \hat{\beta_1}x_i) = 0 
$$


$$
\sum_{i=1}^{n}y_i - N\hat{\beta_0} - N\hat{\beta_1}\sum_{i=1}^{n}x_i = 0 
$$

Dividindo tudo por $N$

$$
\frac{\displaystyle  \sum_{i=1}^{n}y_i}{N} - \frac{\displaystyle  N\hat{\beta_0}}{N}-\frac{\displaystyle  N\hat{\beta_1}\sum_{i=1}^{n}x_i}{N} = \frac{0}{N} 
$$


$$
\bar{y} - \hat{\beta_0} - \hat{\beta_1}\bar{x} = 0 
$$


Com isso descobrimos que $\displaystyle \frac{\partial S}{\partial \hat{\beta_0}}$ vale 0 quando


$$
\hat{\beta_0} = \bar{y} - \hat{\beta_1}\bar{x}
$$


----

----

### Derivada paricla em relação à $\hat{\beta_1}$

A derivada de $S$ em relação a $\hat{\beta_1}$ é

$$
\frac{\partial S}{\partial \hat{\beta_1}} = \sum_{i=1}^{n} (0 + 0 + \hat{\beta_1}^2 x_i^2 - 0 - 2y_i\hat{\beta_1}x_i + 2\hat{\beta_0}\hat{\beta_1}x_i)
$$

$$
\frac{\partial S}{\partial \hat{\beta_1}} = \sum_{i=1}^{n} (0 + 0 + 2\hat{\beta_1}x_i^2 - 0 - 2y_ix_i + 2\hat{\beta_0}x_i)
$$


$$
\frac{\partial S}{\partial \hat{\beta_1}} = \sum_{i=1}^{n} (2\hat{\beta_1}x_i^2 - 2y_ix_i + 2\hat{\beta_0}x_i)
$$


$$
\frac{\partial S}{\partial \hat{\beta_1}} = 2\sum_{i=1}^{n} x_i(\hat{\beta_1}x_i - y_i + \hat{\beta_0})
$$

Essa é a derivada de S em relação a $\hat{\beta_1}$. Agora igualamos a zero para encontrar o ponto de mínima.

$$
\frac{\partial S}{\partial \hat{\beta_1}} = 2\sum_{i=1}^{n} x_i(\hat{\beta_1}x_i - y_i + \hat{\beta_0}) = 0
$$

Dividindo por 2 para simplificação


$$
\frac{\displaystyle  2\sum_{i=1}^{n} x_i(\hat{\beta_1}x_i - y_i + \hat{\beta_0})}{2} = \frac{0}{2}
$$


$$
\sum_{i=1}^{n} x_i(\hat{\beta_1}x_i - y_i + \hat{\beta_0}) = 0
$$


$$
\sum_{i=1}^{n} x_i(-y_i + \hat{\beta_0} + \hat{\beta_1}x_i) = 0
$$


$$
-\sum_{i=1}^{n}x_iy_i + \hat{\beta_0} \sum_{i=1}^{n}x_i + \hat{\beta_1}\sum_{i=1}^{n}x_i^2 = 0
$$

Com isso descobrimos que $\frac{\partial S}{\partial \hat{\beta_1}}$ vale 0 quando

$$
\hat{\beta_0} \sum_{i=1}^{n}x_i + \hat{\beta_1}\sum_{i=1}^{n}x_i^2 = \sum_{i=1}^{n}x_iy_i
$$

----

----

Encontradas as derivadas, ainda falta resolvermos essas equações. A derivada de $S$ em relação à $\hat{\beta_0}$ ainda precisa de $\hat{\beta_1}$ para ser resolvida, e o mesmo ocorre para $\hat{\beta_1}$.

Uma solução para isso é substituir o valor de $\hat{\beta_0}$ por $\displaystyle \bar{y} - \hat{\beta_1}\bar{x}$ na última equação apresentada acima. Essa substituição é valida pois estamos substituindo $\hat{\beta_0}$ pelo seu valor que garante um resultado 0 para sua derivada. 

Substituindo o valor de $\hat{\beta_0}$ por $\displaystyle \bar{y} - \hat{\beta_1}\bar{x}$


$$
(\bar{y}-\hat{\beta_1}\bar{x})\sum_{i=1}^{n}x_i + \hat{\beta_1}\sum_{i=1}^{n}x_i^2 = \sum_{i=1}^{n}x_iy_i
$$

Como $\bar{x} =\displaystyle \frac{\displaystyle \sum_{i=1}^{n}x_i}{N}$, então $\displaystyle \sum_{i=1}^{n}x_i = N\bar{x}$.


$$
(\bar{y} - \hat{\beta_1}\bar{x}) N\bar{x} + \hat{\beta_1}\sum_{i=1}^{n}x_i^2 = \sum_{i=1}^{n}x_iy_i
$$

$$
N\bar{x}\bar{y} - \hat{\beta_1}N\bar{x}^2 + \hat{\beta_1}\sum_{i=1}^{n}x_i^2 = \sum_{i=1}^{n}x_iy_i
$$

$$
-\hat{\beta_1}N\bar{x}^2 + \hat{\beta_1}\sum_{i=1}^{n}x_i^2 = \sum_{i=1}^{n}x_iy_i - N\bar{x}\bar{y}
$$

$$
\hat{\beta_1}(\sum_{i=1}^{n}x_i^2 - N\bar{x}^2) = \sum_{i=1}^{n}x_iy_i - N\bar{x}\bar{y}
$$


$$
\hat{\beta_1} = \frac{\displaystyle \sum_{i=1}^{n}x_iy_i - N\bar{x}\bar{y}}{\displaystyle \sum_{i=1}^{n}x_i^2 - N\bar{x}^2} \qquad ou \qquad \hat{\beta_1} = \frac{\displaystyle \sum_{i=1}^{n}x_iy_i - \frac{\displaystyle \sum_{i=1}^{n}x_i\sum_{i=1}^{n}y_i}{N}}{\displaystyle \sum_{i=1}^{n}x_i^2 - \frac{\displaystyle (\sum_{i=1}^{n}x_i)^2}{N}}
$$

Agora, podemos encontrar o valor de $\hat{\beta_1}$ que garante o menor valor de $S$.

```{r Encontrando os valores, echo = TRUE}
x <- d2h
y <- v

N <- length(x)

#Serão chamados de b0.mqo e b1.mqo, para lembrarmos que são os valores obtido por meio das operações realizadas acima
b1.mqo <- (sum(x*y)-(sum(x)*sum(y))/N)/(sum(x^2)-(sum(x)^2)/N)

b0.mqo <- mean(y) - b1.mqo*mean(x)

b0.mqo
b1.mqo

```


O caso mostrado acima foi montado com base na regressão linear simples, com somente os parâmetros $\hat{\beta_0}$ e $\hat{\beta_1}$. O sistema de equações normais nesse caso é disposto da seguinte forma

$$
\begin{cases} \displaystyle  N\hat{\beta_0} \enspace \enspace \quad + \quad \hat{\beta_1}\sum_{i=1}^{n}x_i \quad = \quad \sum_{i=1}^{n}y_i \\ 
\displaystyle \hat{\beta_0}\sum_{i=1}^{n}x_i \quad + \quad \hat{\beta_1}\sum_{i=1}^{n}x_i^2 \quad = \quad \sum_{i=1}^{n}y_ix_i \end{cases}
$$


Para uma regressão linear múltipla, com mais de uma variável explicativa, o processo é similar, porém adicionando-se mais uma equações ao sistema.

Entretanto, quando muitas variáveis são adicionadas, convém dispor por meio de cálculo matricial. Essa disposição também é muito encontrada na literatura, e é comumente chamada de método análitico de resolução da regressão linear. As variáveis X, Y e os coeficientes assumem matrizes e vetores, como a seguir.

$\hat{\beta}$ é o vetor de parâmetros do modelo, de dimensão $p\times 1$. $Y$ é o vetor da variável observada. Tem dimensão $n \times 1$. $\epsilon$ é o vetor de resíduos, de dimensão $n \times 1$.
$$
\mathbf{\hat{\beta}} = \left[\begin{array}{c}
\hat{\beta_0} \\ \hat{\beta_1} \\ \vdots \\ \hat{\beta_p}
\end{array}\right]_{p\times 1} \qquad \mathbf{Y} = \left[\begin{array}{c}
y_1 \\ y_2 \\ \vdots \\ y_n
\end{array}\right]_{n \times 1} \qquad \mathbf{\epsilon} = \left[\begin{array}{c}
e_1 \\ e_2 \\ \vdots \\ e_n
\end{array}\right]_{n \times 1}
$$


$X$ é a matriz das variáveis explicativas do modelo, onde $n$ é o número de observações, e $p$ é o número de parâmetros do modelo.
$$
\mathbf{X} = \left[\begin{array}
{cccc}   
1 & X_{11} & X_{12} & \dots & X_{1p} \\ 
1 & X_{21} & X_{22} & \dots & X_{2p} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
1 & X_{n1} & X_{n2} & \dots & X{np} \\
\end{array}\right]_{n \times p}
$$




Assim como no processo anterior, é desejado determinar o menor valor da soma de quadrado dos resíduos.

$$
S = \sum_{i=1}^{n} e_i^2 = \epsilon^\top \epsilon
$$

$$
\epsilon^\top \epsilon = (Y - \hat{\beta}X)^\top (Y - X\hat{\beta})
$$

$$
= Y^\top Y - Y^\top (X\hat{\beta}) - (X\hat{\beta})^\top Y + (X\hat{\beta})^\top (X\hat{\beta})
$$
$$
= Y^\top Y - (X\hat{\beta})^\top Y - (X\hat{\beta})^\top Y + (X\hat{\beta})^\top (X\hat{\beta})
$$
$$
= Y^\top Y - 2(X\hat{\beta})^\top Y + (X\hat{\beta})^\top (X\hat{\beta})
$$
$$
 \mathbf{S} = Y^\top Y - 2\hat{\beta}^\top X^\top Y + \hat{\beta}^\top X^\top X \hat{\beta}
$$

A derivada parcial é dada por 

$$
\frac{\partial}{\partial \hat{\beta}}[\epsilon^\top \epsilon] = 0 \enspace - \enspace 2\hat{\beta}^\top X^\top Y \enspace + \enspace \hat{\beta}^\top X^\top X\hat{\beta} \enspace = \enspace 0
$$


$$
0 - 2X^\top Y + 2X^\top X\hat{\beta} = 0
$$


$$
2X^\top X\hat{\beta} = 2X^\top Y 
$$

$$
X^\top X\hat{\beta} = X^\top Y 
$$


$$
\mathbf{\hat{\beta}} = (X^\top X)^{-1} X^\top Y 
$$

Essa é a equação que garante o menor valor de $S$. Também podemos utilizá-la para encontrar os parâmetros.

```{r analitico, echo = TRUE}
X <- model.matrix(y ~ x)

betas <- solve(t(X) %*% X) %*% t(X) %*% y

betas

```


E só para termos certeza de que tudo foi feito corretamente.
```{r garantia, echo = TRUE}
coef(lm(y ~ x))
```

As três tentativas garantem o mesmo resultado.

```{r comparacao entre metodos}
df.comparacao <- data.frame("MQO" = c(b0.mqo,b1.mqo),
                            "Mat" = c(betas[1],betas[2]),
                            "R" = c(coef(lm(y~x))[1],coef(lm(y~x))[2]))
rownames(df.comparacao) <- c("B0","B1")


df.comparacao %>% kable(col.names = c("MQO", "MQO_Mat","R")) %>% kable_styling(bootstrap_options=c("condensed","striped","hover"), position = "center",full_width = FALSE)
```


Esse é o método dos mínimos quadrados.
























