<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="Gabriel Orso" />


<title>Mínimos Quadrados Ordinários</title>

<script src="site_libs/header-attrs-2.10/header-attrs.js"></script>
<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/flatly.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<script src="site_libs/navigation-1.1/codefolding.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<script src="site_libs/kePrint-0.0.1/kePrint.js"></script>
<link href="site_libs/lightable-0.0.1/lightable.css" rel="stylesheet" />
<link href="site_libs/font-awesome-5.1.0/css/all.css" rel="stylesheet" />
<link href="site_libs/font-awesome-5.1.0/css/v4-shims.css" rel="stylesheet" />
<link rel="icon" type="image/ico" href="favicon/tree.ico"/>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>





<link rel="stylesheet" href="styles.css" type="text/css" />



<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.tab('show');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->
<style type="text/css">
.code-folding-btn { margin-bottom: 4px; }
</style>



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html"></a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="ggplot.html">ggplot</a>
</li>
<li>
  <a href="MQO.html">MQO</a>
</li>
<li>
  <a href="dplyr.html">dplyr</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="https://github.com/gabrielorso/gabrielorso.github.io">
    <span class="fa fa-github fa-lg"></span>
     
    GitHub
  </a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">

<div class="btn-group pull-right float-right">
<button type="button" class="btn btn-default btn-xs btn-secondary btn-sm dropdown-toggle" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false"><span>Code</span> <span class="caret"></span></button>
<ul class="dropdown-menu dropdown-menu-right" style="min-width: 50px;">
<li><a id="rmd-show-all-code" href="#">Show All Code</a></li>
<li><a id="rmd-hide-all-code" href="#">Hide All Code</a></li>
</ul>
</div>



<h1 class="title toc-ignore">Mínimos Quadrados Ordinários</h1>
<h4 class="author">Gabriel Orso</h4>

</div>


<pre class="r"><code>library(ggplot2)
library(gganimate)
library(magrittr)
library(dplyr)
library(kableExtra)</code></pre>
<p>Utilizaremos o banco de dados <strong>trees</strong>. Esse banco de dados contém informações sobre o diâmetro em polegadas, altura em pés e volume em pés³ de 31 árvores <em>black cherry</em>.</p>
<p>Além disso, os gráficos serão gerados utilizando o pacote <code>ggplot2</code>, com comentários pertinentes sobre as funções utilizadas. Os comandos do pacote <code>ggplot2</code> são inseridos em camadas, normalmente iniciando por uma função que indica o banco de dados utilizado, e posteriormente as demais funções. As variáveis que serão utilizadas para criar os gráficos devem sempre estar “mapeadas” para serem encontradas, e a maneira de fazer isso é inserindo-as dentro do argumento <code>aes()</code>. No exemplo abaixo, o primeiro nível de comando especifica o banco de dados utilizado. O segundo indica o tipo de gráfico a ser criado e mapeia as variáveis de interesse. O terceiro comando introduz os títulos dos eixos, do gráfico, etc. E o quarto comando aplica um tema pronto ao gráfico. Para mais informações sobre pacote <code>ggplot</code>, acesse esse <a href="ggplot.html">breve tutorial</a>.</p>
<pre class="r"><code>data(&quot;trees&quot;)

# Transforma as variáveis para unidades comumente utilizadas
d &lt;- trees$Girth/2.54     # polegadas para centímetros
ht &lt;- trees$Height/3.281  # pés para metros
v &lt;- trees$Volume/34.286  # pés cúbicos para metros cúbicos
d2h &lt;- d^2*ht             

# &#39;d&#39; é um vetor de diâmetros, e possui 31 elementos. Ou length(d)
d</code></pre>
<pre><code>##  [1] 3.267717 3.385827 3.464567 4.133858 4.212598 4.251969 4.330709 4.330709
##  [9] 4.370079 4.409449 4.448819 4.488189 4.488189 4.606299 4.724409 5.078740
## [17] 5.078740 5.236220 5.393701 5.433071 5.511811 5.590551 5.708661 6.299213
## [25] 6.417323 6.811024 6.889764 7.047244 7.086614 7.086614 8.110236</code></pre>
<pre class="r"><code>length(d)</code></pre>
<pre><code>## [1] 31</code></pre>
<pre class="r"><code># O mesmo ocorre com ht, v e d2h.

# Agrupa tudo em um único data frame
dados &lt;- data.frame(d,ht,v,d2h)

g.base &lt;- ggplot(data = dados, 
                 aes(x = d2h, y = v)) +     #Introduz o dataframe utilizado
  
  geom_point() + #geom_point() cria gráfico de pontos
  
  labs(x = &quot;d2h&quot;,          #Adiciona Títulos dos eixos e do gráfico
       y = &quot;Volume (m³)&quot;,
       title = &quot;Volume em função de d2h&quot;) + 
  
  theme_bw()                #Adiciona o tema Black and White


plot(g.base)</code></pre>
<p><img src="MQO_files/figure-html/Banco%20de%20dados-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>A figura acima demonstra a relação entre volume e <span class="math inline">\(d^2h\)</span>. Essa relação é conhecida no meio florestal através do modelo de Spurr: <span class="math inline">\(v = \beta_0 + \beta_1.d^2h\)</span>, e evidencia a relação linear entre as duas variáveis.</p>
<p>Para encontrar os parâmetros do modelo, basta utilizar a função <code>lm()</code>.</p>
<pre class="r"><code>spurr &lt;- lm(v ~ d2h, data = dados)

summary(spurr)</code></pre>
<pre><code>## 
## Call:
## lm(formula = v ~ d2h, data = dados)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.13473 -0.03209 -0.00483  0.05090  0.12243 
## 
## Coefficients:
##                Estimate  Std. Error t value            Pr(&gt;|t|)    
## (Intercept) -0.00868224  0.02810346  -0.309                0.76    
## d2h          0.00131156  0.00003673  35.711 &lt;0.0000000000000002 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.07271 on 29 degrees of freedom
## Multiple R-squared:  0.9778, Adjusted R-squared:  0.977 
## F-statistic:  1275 on 1 and 29 DF,  p-value: &lt; 0.00000000000000022</code></pre>
<pre class="r"><code>coef(spurr)</code></pre>
<pre><code>##  (Intercept)          d2h 
## -0.008682245  0.001311559</code></pre>
<pre class="r"><code>g.base + labs(title = &quot;Modelo de Spurr Ajustado&quot;) +
  geom_abline(intercept = coef(spurr)[1],  #Coeficiente linear  (b0)
              slope = coef(spurr)[2],      #Coeficiente angular (b1)
              color = &quot;steelblue&quot;) </code></pre>
<p><img src="MQO_files/figure-html/linear%20simples-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Os valores de <span class="math inline">\(\beta_0\)</span> e <span class="math inline">\(\beta_1\)</span> encontrados foram de -0.0087 e 0.0013, respectivamente.</p>
<p>Agora vamos tentar chegar nos mesmos parâmetros realizando o método dos mínimos quadrados ordinários passo a passo.</p>
<div id="mínimos-quadrados-ordinários" class="section level2">
<h2>Mínimos Quadrados Ordinários</h2>
<p>O método dos mínimos quadrados ordinários é o principal método de estimativa dos parâmetros para regressões lineares, e tem como objetivo encontrar os parâmetros do modelo que resultem na menor soma de quadrados dos resíduos. Sua utilização depende de pressuposições que devem ser satisfeitas. São elas:</p>
<ul>
<li><strong>1.</strong> Erro com média 0 e distribuição normal;</li>
<li><strong>2.</strong> Variância do erro constante;</li>
<li><strong>3.</strong> Independência dos resíduos.</li>
</ul>
<p>A formulação tradicional do modelo linear simples é exemplificada abaixo,</p>
<p><span class="math display">\[
y = \beta_0 + \beta_1x + \epsilon
\]</span></p>
<p>Onde <span class="math inline">\(\beta_0\)</span> é o parâmetro linear, também chamado de intercepto ou constante, pois não depende de <span class="math inline">\(x\)</span>; <span class="math inline">\(\beta_1\)</span> é o parâmetro angular ou coeficiente da variável <span class="math inline">\(x\)</span>; <span class="math inline">\(\epsilon\)</span> é o erro, e representa a variação de <span class="math inline">\(y\)</span> que não é explicada pelo modelo. Outra formulação encontrada na literatura é <span class="math inline">\(y = \alpha + \beta x + \epsilon\)</span>.</p>
<p>Os valores estimados ou preditos da variável <span class="math inline">\(y\)</span> são denotados por <span class="math inline">\(\hat{y}\)</span>,</p>
<p><span class="math display">\[
\hat{y} = \hat{\beta_0} + \hat{\beta_1}x 
\]</span></p>
<p>Os parâmetros <em>verdadeiros</em> <span class="math inline">\(\beta_i\)</span> do modelo são desconhecidos, e em vez disso se utiliza <span class="math inline">\(\hat{\beta_0}\)</span> e <span class="math inline">\(\hat{\beta_1}\)</span> para definir os parâmetros <em>estimados</em> pela regressão.</p>
<p>O erro <span class="math inline">\(\epsilon\)</span> nada mais é que a diferença entre o valor real observado <span class="math inline">\(y\)</span>, e o valor estimado ou predito <span class="math inline">\(\hat{y}\)</span>.</p>
<p><span class="math display">\[
\epsilon_i = y_i - \hat{y_i}
\]</span></p>
<p>Como citado anteriormente, o objetivo do método é encontrar os parâmetros que resultem na menor soma de quadrados de resíduos. Como os resíduos devem possuir média zero e distribuição normal, alguns resíduos são positivos e outros negativos. Nessa situação, a simples soma teria efeito de ‘compensação’ entre valores positivos e negativos, e o resultado final não representaria de forma apropriada o erro do modelo (o somatório seria próximo de zero). Daí a necessidade de elevar cada erro <span class="math inline">\(\epsilon\)</span> ao quadrado.</p>
<p><span class="math display">\[
S = \sum_{i=1}^{n} \epsilon_i^2 = min
\]</span></p>
<p>Com a substituição de alguns termos com as equações apresentadas acima, temos que</p>
<p><span class="math display">\[
S = \sum_{i=1}^{n} (y_i - \hat{y_i})^2
\]</span></p>
<p><span class="math display">\[
S = \sum_{i=1}^{n} (y_i - [\hat{\beta_0} + \hat{\beta_1}x_i])^2
\]</span></p>
<p><span class="math display">\[
S = \sum_{i=1}^{n} (y_i - \hat{\beta_0} - \hat{\beta_1}x_i)^2 = min
\]</span></p>
<p>Agora, se atribuíssemos algum valor qualquer para os parâmetros no modelo acima e calculássemos o somatório, isso resultaria em um valor S de Soma de Quadrados de Resíduos. Conforme os valores dos parâmetros mudam, então também a Soma de Quadrados de Resíduos (S) irá mudar.</p>
<pre class="r"><code>#Cria uma função S que recebe os parâmetros x,y,a,b
S &lt;- function(x,y,b0,b1) {sum((y-b0-b1*x)^2)}

#Essa função criada recebe 4 argumentos: um vetor x, um vetor y, um valor de b0 e um valor de b1. Como x e y são dois vetores e b0 e b1 são dois valores, o que a função fará é realizar a operação y - b0 - b1*x para os primeiros valores de x e y e elevar o resultado ao quadrado. A seguir, realiza a mesma operação para os segundos valores e eleva o resultado ao quadrado. Por fim, é requisitado por meio da função sum() que faça o somatório de todos esses resultados. Por fim, isso resultará em um único valor que é a soma do quadrado dos resíduos para aquele par de coeficientes b0 e b1 informados.

#Cria um vetor de possíveis valores de b0 e b1
b_0 &lt;- seq(from = -0.5, to = 0.5, by = 0.01)  #De -0.5 até 0.5, ao passo de 0.01
b_1 &lt;- seq(from = -0.5, to = 0.5, by = 0.01)

#Isso resulta em 101 valores diferentes de b0 e b1.
length(b_0) ; length(b_1)</code></pre>
<pre><code>## [1] 101</code></pre>
<pre><code>## [1] 101</code></pre>
<pre class="r"><code>#Cria vetor que receberá o resultado de S para cada possível valor de b0 e b1. Naturalmente, deve ter o mesmo comprimento do número de possíveis valores dos coeficientes.
S.est &lt;- vector(length = length(b_0))  #Vetor de comprimento igual ao vetor b0

#Cria um loop para estimar cada valor de S baseado em cada valor de b0 e b1
for (i in 1:length(S.est)) {
  
  S.est[i] &lt;- S(x = dados$d2h,  
                y = dados$v,
                b0 = b_0[i],
                b1 = b_1[i])
}

#O comando acima pega a função S() criada acima, joga os primeiros valores de b0 e b1 dos vetores, calcula o valor de S e o guarda na primeira posição do vetor S.est. A seguir, pega todos valores das segundas posições dos vetores, joga na função S() criada e resgata o valor resultante, salvando na segunda posição do vetor S.est. Esse processo é repetido de 1:length(S.est), ou seja, do primeiro até o último valor dos vetores.


#Criando um data frame com as variáveis
df &lt;- data.frame(&quot;b0&quot; = b_0,
                 &quot;b1&quot; = b_1,
                 &quot;S&quot; = S.est)

g.S &lt;- ggplot(data = df, aes(x = b0, y = S)) + 
  geom_line(aes(color = S.est), size = 1) +  # Cria uma reta conectando cada observação. Como são 101 pontos, o resultado terá a forma de uma parábola.
  labs(x = &quot;Parâmetro b0&quot;, y = &quot;S (m³)²&quot;, subtitle = &quot;Efeito de b0 na Soma de Quadrado dos Resíduos&quot;) + 
  scale_color_gradient(low = &quot;blue&quot;, high = &quot;red&quot;, name = &quot;S (m³)²&quot;) + 
  geom_segment(aes(x=-0.2,xend=0.2, y = min(S), yend = min(S)), color = &quot;black&quot;) +
  theme_bw()



plot(g.S)</code></pre>
<p><img src="MQO_files/figure-html/Varia%C3%A7%C3%A3o%20de%20S-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Na figura acima, percebe-se que quando <span class="math inline">\(\hat{\beta_0}\)</span> é próximo de -0.5, a soma de quadrado dos resíduos é alta, assim como perto de 0.5. Entretanto, quando <span class="math inline">\(\hat{\beta_0}\)</span> é próximo de 0, S atinge os menores valores. Se fizéssemos esse procedimento com <span class="math inline">\(\hat{\beta_1}\)</span>, esse mesmo comportamento seria observado. Em outras palavras, existe um determinado valor de <span class="math inline">\(\hat{\beta_0}\)</span> e <span class="math inline">\(\hat{\beta_1}\)</span> que resultam no valor mínimo de S. Para encontrar os valores exatos, podemos fazer uso do cálculo diferencial.</p>
<p>Analisando a figura, percebemos que o ponto que resulta no menor valor de S é um ponto de mínima, e se trassarmos uma reta tangente à curva nesse ponto, essa reta será perfeitamente horizontal.</p>
<p>Uma maneira adequada para encontrar pontos críticos em uma função é utilizando cálculo diferencial, sabendo que nos pontos onde a primeira derivada for igual a zero, então a tangente da função será perfeitamente horizontal, marcando assim um ponto de máxima ou mínima.</p>
<p>Para encontrarmos os valores de <span class="math inline">\(\hat{\beta_0}\)</span> e <span class="math inline">\(\hat{\beta_1}\)</span> que minimizam S, precisamos derivar a função S em relação a esses parâmetros. Esse procedimento é chamado de derivada parcial (denotado por <span class="math inline">\(\partial\)</span>), e em linhas gerais tudo que não possuir relação com o parâmetro alvo é considerado como constante.</p>
<p>Primeiramente podemos abrir o polinômio da função <span class="math inline">\(S\)</span>.</p>
<p><span class="math display">\[
S = \sum_{i=1}^{n} (y_i - \hat{\beta_0} - \hat{\beta_1}x_i)^2
\]</span></p>
<p><span class="math display">\[
S = \sum_{i=1}^{n} (y_i^2 - y_i\hat{\beta_0} - y_i\hat{\beta_i}x_i-y_i\hat{\beta_0} + \hat{\beta_0}^2 + \hat{\beta_0}\hat{\beta_1}x_i - y_i\hat{\beta_1}x_i + \hat{\beta_0}\hat{\beta_1}x_i + \hat{\beta_1}^2x_i^2)
\]</span></p>
<p><span class="math display">\[
S = \sum_{i=1}^{n} (y_i^2 + \hat{\beta_0}^2 + \hat{\beta_1}^2x_i^2 -2y_i\hat{\beta_0} - 2y_i\hat{\beta_1}x_i + 2\hat{\beta_0}\hat{\beta_1}x_i)
\]</span></p>
<hr />
<hr />
<div id="derivada-parcial-em-relação-à-hatbeta_0" class="section level3">
<h3>Derivada parcial em relação à <span class="math inline">\(\hat{\beta_0}\)</span></h3>
<p>As regras de derivação são as mesmas de qualquer outra diferenciação. A única diferença é que, agora, a variável de interesse é <span class="math inline">\(\hat{\beta_0}\)</span>, e tudo o mais é considerado constante.</p>
<p><span class="math display">\[
\frac{\partial S}{\partial \hat{\beta_0}} = \sum_{i=1}^{n} (0 + \hat{\beta_0}^2 + 0 - 2y_i\hat{\beta_0} - 0 + 2\hat{\beta_0}\hat{\beta_1}x_i)
\]</span></p>
<p><span class="math display">\[
\frac{\partial S}{\partial \hat{\beta_0}} = \sum_{i=1}^{n} (0 + 2\hat{\beta_0} + 0 - 2y_i - 0 + 2\hat{\beta_1}x_i)
\]</span></p>
<p><span class="math display">\[
\frac{\partial S}{\partial \hat{\beta_0}} = \sum_{i=1}^{n} (2\hat{\beta_0} - 2y_i + 2\hat{\beta_1}x_i) 
\]</span></p>
<p>Igualando a 0</p>
<p><span class="math display">\[
\frac{\partial S}{\partial \hat{\beta_0}} = \sum_{i=1}^{n} (2\hat{\beta_0} - 2y_i + 2\hat{\beta_1}x_i) = 0
\]</span></p>
<p><span class="math display">\[
\frac{\partial S}{\partial \hat{\beta_0}} = 2\sum_{i=1}^{n} (\hat{\beta_0} - y_i + \hat{\beta_1}x_i) = 0
\]</span></p>
<p>Podemos ainda realizar algumas simplificações.</p>
<p>Dividindo por 2 e multiplicando por -1</p>
<p><span class="math display">\[
\frac{\displaystyle  2\sum_{i=1}^{n} (\hat{\beta_0} - y_i + \hat{\beta_1}x_i)}{2} (-1) = \frac{0}{2}(-1)
\]</span></p>
<p><span class="math display">\[
\sum_{i=1}^{n} (y_i - \hat{\beta_0} - \hat{\beta_1}x_i) = 0 
\]</span></p>
<p><span class="math display">\[
\sum_{i=1}^{n}y_i - N\hat{\beta_0} - N\hat{\beta_1}\sum_{i=1}^{n}x_i = 0 
\]</span></p>
<p>Dividindo tudo por <span class="math inline">\(N\)</span></p>
<p><span class="math display">\[
\frac{\displaystyle  \sum_{i=1}^{n}y_i}{N} - \frac{\displaystyle  N\hat{\beta_0}}{N}-\frac{\displaystyle  N\hat{\beta_1}\sum_{i=1}^{n}x_i}{N} = \frac{0}{N} 
\]</span></p>
<p><span class="math display">\[
\bar{y} - \hat{\beta_0} - \hat{\beta_1}\bar{x} = 0 
\]</span></p>
<p>Com isso descobrimos que <span class="math inline">\(\displaystyle \frac{\partial S}{\partial \hat{\beta_0}}\)</span> vale 0 quando</p>
<p><span class="math display">\[
\hat{\beta_0} = \bar{y} - \hat{\beta_1}\bar{x}
\]</span></p>
<hr />
<hr />
</div>
<div id="derivada-paricla-em-relação-à-hatbeta_1" class="section level3">
<h3>Derivada paricla em relação à <span class="math inline">\(\hat{\beta_1}\)</span></h3>
<p>A derivada de <span class="math inline">\(S\)</span> em relação a <span class="math inline">\(\hat{\beta_1}\)</span> é</p>
<p><span class="math display">\[
\frac{\partial S}{\partial \hat{\beta_1}} = \sum_{i=1}^{n} (0 + 0 + \hat{\beta_1}^2 x_i^2 - 0 - 2y_i\hat{\beta_1}x_i + 2\hat{\beta_0}\hat{\beta_1}x_i)
\]</span></p>
<p><span class="math display">\[
\frac{\partial S}{\partial \hat{\beta_1}} = \sum_{i=1}^{n} (0 + 0 + 2\hat{\beta_1}x_i^2 - 0 - 2y_ix_i + 2\hat{\beta_0}x_i)
\]</span></p>
<p><span class="math display">\[
\frac{\partial S}{\partial \hat{\beta_1}} = \sum_{i=1}^{n} (2\hat{\beta_1}x_i^2 - 2y_ix_i + 2\hat{\beta_0}x_i)
\]</span></p>
<p><span class="math display">\[
\frac{\partial S}{\partial \hat{\beta_1}} = 2\sum_{i=1}^{n} x_i(\hat{\beta_1}x_i - y_i + \hat{\beta_0})
\]</span></p>
<p>Essa é a derivada de S em relação a <span class="math inline">\(\hat{\beta_1}\)</span>. Agora igualamos a zero para encontrar o ponto de mínima.</p>
<p><span class="math display">\[
\frac{\partial S}{\partial \hat{\beta_1}} = 2\sum_{i=1}^{n} x_i(\hat{\beta_1}x_i - y_i + \hat{\beta_0}) = 0
\]</span></p>
<p>Dividindo por 2 para simplificação</p>
<p><span class="math display">\[
\frac{\displaystyle  2\sum_{i=1}^{n} x_i(\hat{\beta_1}x_i - y_i + \hat{\beta_0})}{2} = \frac{0}{2}
\]</span></p>
<p><span class="math display">\[
\sum_{i=1}^{n} x_i(\hat{\beta_1}x_i - y_i + \hat{\beta_0}) = 0
\]</span></p>
<p><span class="math display">\[
\sum_{i=1}^{n} x_i(-y_i + \hat{\beta_0} + \hat{\beta_1}x_i) = 0
\]</span></p>
<p><span class="math display">\[
-\sum_{i=1}^{n}x_iy_i + \hat{\beta_0} \sum_{i=1}^{n}x_i + \hat{\beta_1}\sum_{i=1}^{n}x_i^2 = 0
\]</span></p>
<p>Com isso descobrimos que <span class="math inline">\(\frac{\partial S}{\partial \hat{\beta_1}}\)</span> vale 0 quando</p>
<p><span class="math display">\[
\hat{\beta_0} \sum_{i=1}^{n}x_i + \hat{\beta_1}\sum_{i=1}^{n}x_i^2 = \sum_{i=1}^{n}x_iy_i
\]</span></p>
<hr />
<hr />
<p>Encontradas as derivadas, ainda falta resolvermos essas equações. A derivada de <span class="math inline">\(S\)</span> em relação à <span class="math inline">\(\hat{\beta_0}\)</span> ainda precisa de <span class="math inline">\(\hat{\beta_1}\)</span> para ser resolvida, e o mesmo ocorre para <span class="math inline">\(\hat{\beta_1}\)</span>.</p>
<p>Uma solução para isso é substituir o valor de <span class="math inline">\(\hat{\beta_0}\)</span> por <span class="math inline">\(\displaystyle \bar{y} - \hat{\beta_1}\bar{x}\)</span> na última equação apresentada acima. Essa substituição é valida pois estamos substituindo <span class="math inline">\(\hat{\beta_0}\)</span> pelo seu valor que garante um resultado 0 para sua derivada.</p>
<p>Substituindo o valor de <span class="math inline">\(\hat{\beta_0}\)</span> por <span class="math inline">\(\displaystyle \bar{y} - \hat{\beta_1}\bar{x}\)</span></p>
<p><span class="math display">\[
(\bar{y}-\hat{\beta_1}\bar{x})\sum_{i=1}^{n}x_i + \hat{\beta_1}\sum_{i=1}^{n}x_i^2 = \sum_{i=1}^{n}x_iy_i
\]</span></p>
<p>Como <span class="math inline">\(\bar{x} =\displaystyle \frac{\displaystyle \sum_{i=1}^{n}x_i}{N}\)</span>, então <span class="math inline">\(\displaystyle \sum_{i=1}^{n}x_i = N\bar{x}\)</span>.</p>
<p><span class="math display">\[
(\bar{y} - \hat{\beta_1}\bar{x}) N\bar{x} + \hat{\beta_1}\sum_{i=1}^{n}x_i^2 = \sum_{i=1}^{n}x_iy_i
\]</span></p>
<p><span class="math display">\[
N\bar{x}\bar{y} - \hat{\beta_1}N\bar{x}^2 + \hat{\beta_1}\sum_{i=1}^{n}x_i^2 = \sum_{i=1}^{n}x_iy_i
\]</span></p>
<p><span class="math display">\[
-\hat{\beta_1}N\bar{x}^2 + \hat{\beta_1}\sum_{i=1}^{n}x_i^2 = \sum_{i=1}^{n}x_iy_i - N\bar{x}\bar{y}
\]</span></p>
<p><span class="math display">\[
\hat{\beta_1}(\sum_{i=1}^{n}x_i^2 - N\bar{x}^2) = \sum_{i=1}^{n}x_iy_i - N\bar{x}\bar{y}
\]</span></p>
<p><span class="math display">\[
\hat{\beta_1} = \frac{\displaystyle \sum_{i=1}^{n}x_iy_i - N\bar{x}\bar{y}}{\displaystyle \sum_{i=1}^{n}x_i^2 - N\bar{x}^2} \qquad ou \qquad \hat{\beta_1} = \frac{\displaystyle \sum_{i=1}^{n}x_iy_i - \frac{\displaystyle \sum_{i=1}^{n}x_i\sum_{i=1}^{n}y_i}{N}}{\displaystyle \sum_{i=1}^{n}x_i^2 - \frac{\displaystyle (\sum_{i=1}^{n}x_i)^2}{N}}
\]</span></p>
<p>Agora, podemos encontrar o valor de <span class="math inline">\(\hat{\beta_1}\)</span> que garante o menor valor de <span class="math inline">\(S\)</span>.</p>
<pre class="r"><code>x &lt;- d2h
y &lt;- v

N &lt;- length(x)

#Serão chamados de b0.mqo e b1.mqo, para lembrarmos que são os valores obtido por meio das operações realizadas acima
b1.mqo &lt;- (sum(x*y)-(sum(x)*sum(y))/N)/(sum(x^2)-(sum(x)^2)/N)

b0.mqo &lt;- mean(y) - b1.mqo*mean(x)

b0.mqo</code></pre>
<pre><code>## [1] -0.008682245</code></pre>
<pre class="r"><code>b1.mqo</code></pre>
<pre><code>## [1] 0.001311559</code></pre>
<p>O caso mostrado acima foi montado com base na regressão linear simples, com somente os parâmetros <span class="math inline">\(\hat{\beta_0}\)</span> e <span class="math inline">\(\hat{\beta_1}\)</span>. O sistema de equações normais nesse caso é disposto da seguinte forma</p>
<p><span class="math display">\[
\begin{cases} \displaystyle  N\hat{\beta_0} \enspace \enspace \quad + \quad \hat{\beta_1}\sum_{i=1}^{n}x_i \quad = \quad \sum_{i=1}^{n}y_i \\ 
\displaystyle \hat{\beta_0}\sum_{i=1}^{n}x_i \quad + \quad \hat{\beta_1}\sum_{i=1}^{n}x_i^2 \quad = \quad \sum_{i=1}^{n}y_ix_i \end{cases}
\]</span></p>
<p>Para uma regressão linear múltipla, com mais de uma variável explicativa, o processo é similar, porém adicionando-se mais uma equações ao sistema.</p>
<p>Entretanto, quando muitas variáveis são adicionadas, convém dispor por meio de cálculo matricial. Essa disposição também é muito encontrada na literatura, e é comumente chamada de método análitico de resolução da regressão linear. As variáveis X, Y e os coeficientes assumem matrizes e vetores, como a seguir.</p>
<p><span class="math inline">\(\hat{\beta}\)</span> é o vetor de parâmetros do modelo, de dimensão <span class="math inline">\(p\times 1\)</span>. <span class="math inline">\(Y\)</span> é o vetor da variável observada. Tem dimensão <span class="math inline">\(n \times 1\)</span>. <span class="math inline">\(\epsilon\)</span> é o vetor de resíduos, de dimensão <span class="math inline">\(n \times 1\)</span>. <span class="math display">\[
\mathbf{\hat{\beta}} = \left[\begin{array}{c}
\hat{\beta_0} \\ \hat{\beta_1} \\ \vdots \\ \hat{\beta_p}
\end{array}\right]_{p\times 1} \qquad \mathbf{Y} = \left[\begin{array}{c}
y_1 \\ y_2 \\ \vdots \\ y_n
\end{array}\right]_{n \times 1} \qquad \mathbf{\epsilon} = \left[\begin{array}{c}
e_1 \\ e_2 \\ \vdots \\ e_n
\end{array}\right]_{n \times 1}
\]</span></p>
<p><span class="math inline">\(X\)</span> é a matriz das variáveis explicativas do modelo, onde <span class="math inline">\(n\)</span> é o número de observações, e <span class="math inline">\(p\)</span> é o número de parâmetros do modelo. <span class="math display">\[
\mathbf{X} = \left[\begin{array}
{cccc}   
1 &amp; X_{11} &amp; X_{12} &amp; \dots &amp; X_{1p} \\ 
1 &amp; X_{21} &amp; X_{22} &amp; \dots &amp; X_{2p} \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
1 &amp; X_{n1} &amp; X_{n2} &amp; \dots &amp; X{np} \\
\end{array}\right]_{n \times p}
\]</span></p>
<p>Assim como no processo anterior, é desejado determinar o menor valor da soma de quadrado dos resíduos.</p>
<p><span class="math display">\[
S = \sum_{i=1}^{n} e_i^2 = \epsilon^\top \epsilon
\]</span></p>
<p><span class="math display">\[
\epsilon^\top \epsilon = (Y - \hat{\beta}X)^\top (Y - X\hat{\beta})
\]</span></p>
<p><span class="math display">\[
= Y^\top Y - Y^\top (X\hat{\beta}) - (X\hat{\beta})^\top Y + (X\hat{\beta})^\top (X\hat{\beta})
\]</span> <span class="math display">\[
= Y^\top Y - (X\hat{\beta})^\top Y - (X\hat{\beta})^\top Y + (X\hat{\beta})^\top (X\hat{\beta})
\]</span> <span class="math display">\[
= Y^\top Y - 2(X\hat{\beta})^\top Y + (X\hat{\beta})^\top (X\hat{\beta})
\]</span> <span class="math display">\[
 \mathbf{S} = Y^\top Y - 2\hat{\beta}^\top X^\top Y + \hat{\beta}^\top X^\top X \hat{\beta}
\]</span></p>
<p>A derivada parcial é dada por</p>
<p><span class="math display">\[
\frac{\partial}{\partial \hat{\beta}}[\epsilon^\top \epsilon] = 0 \enspace - \enspace 2\hat{\beta}^\top X^\top Y \enspace + \enspace \hat{\beta}^\top X^\top X\hat{\beta} \enspace = \enspace 0
\]</span></p>
<p><span class="math display">\[
0 - 2X^\top Y + 2X^\top X\hat{\beta} = 0
\]</span></p>
<p><span class="math display">\[
2X^\top X\hat{\beta} = 2X^\top Y 
\]</span></p>
<p><span class="math display">\[
X^\top X\hat{\beta} = X^\top Y 
\]</span></p>
<p><span class="math display">\[
\mathbf{\hat{\beta}} = (X^\top X)^{-1} X^\top Y 
\]</span></p>
<p>Essa é a equação que garante o menor valor de <span class="math inline">\(S\)</span>. Também podemos utilizá-la para encontrar os parâmetros.</p>
<pre class="r"><code>X &lt;- model.matrix(y ~ x)

betas &lt;- solve(t(X) %*% X) %*% t(X) %*% y

betas</code></pre>
<pre><code>##                     [,1]
## (Intercept) -0.008682245
## x            0.001311559</code></pre>
<p>E só para termos certeza de que tudo foi feito corretamente.</p>
<pre class="r"><code>coef(lm(y ~ x))</code></pre>
<pre><code>##  (Intercept)            x 
## -0.008682245  0.001311559</code></pre>
<p>As três tentativas garantem o mesmo resultado.</p>
<table class="table table-condensed table-striped table-hover" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
MQO
</th>
<th style="text-align:right;">
MQO_Mat
</th>
<th style="text-align:right;">
R
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
B0
</td>
<td style="text-align:right;">
-0.0086822
</td>
<td style="text-align:right;">
-0.0086822
</td>
<td style="text-align:right;">
-0.0086822
</td>
</tr>
<tr>
<td style="text-align:left;">
B1
</td>
<td style="text-align:right;">
0.0013116
</td>
<td style="text-align:right;">
0.0013116
</td>
<td style="text-align:right;">
0.0013116
</td>
</tr>
</tbody>
</table>
<p>Esse é o método dos mínimos quadrados.</p>
</div>
</div>

<center>
  <hr width="100%" size="3px">
  <p> <a href="https://creativecommons.org/licenses/by-nc-sa/3.0/deed.pt_BR">
      <img src="img/CC-by-nc-sa80x15.png" alt="Licença Creative Commons 3.0"> </a>
  </p>
  <p> <font size="2"> Este conteúdo
      está disponível por meio da Licença Creative Commons 3.0 </font>
  </p>
</center>


</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->
<script>
$(document).ready(function () {
  window.initializeCodeFolding("show" === "show");
});
</script>

<script>
$(document).ready(function ()  {

    // temporarily add toc-ignore selector to headers for the consistency with Pandoc
    $('.unlisted.unnumbered').addClass('toc-ignore')

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
